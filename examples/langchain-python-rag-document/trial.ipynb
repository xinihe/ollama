{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入LangChain库中的多个模块，这些模块用于加载文档、创建向量存储、嵌入、构建提示模板、连接LLM（大型语言模型）以及管理回调  \n",
    "from langchain.document_loaders import OnlinePDFLoader  # 用于从在线源加载PDF文档  \n",
    "from langchain.vectorstores import Chroma  # 用于创建基于向量的文档存储  \n",
    "from langchain.embeddings import GPT4AllEmbeddings  # 用于嵌入文本的模型  \n",
    "from langchain import PromptTemplate  # 用于构建LLM的提示模板  \n",
    "from langchain.llms import Ollama  # 特定的大型语言模型（LLM）接口  \n",
    "from langchain.callbacks.manager import CallbackManager  # 用于管理回调的类  \n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # 回调处理器，用于将输出流式传输到标准输出  \n",
    "from langchain.chains import RetrievalQA  # 用于实现检索式问答的链  \n",
    "  \n",
    "import sys  # 用于访问与Python解释器紧密相关的变量和函数  \n",
    "import os  # 提供了许多与操作系统交互的功能  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个上下文管理器类，用于临时抑制标准输出和标准错误输出  \n",
    "class SuppressStdout:  \n",
    "    def __enter__(self):  \n",
    "        # 保存原始的标准输出和标准错误输出  \n",
    "        self._original_stdout = sys.stdout  \n",
    "        self._original_stderr = sys.stderr  \n",
    "        # 将标准输出和标准错误输出重定向到/dev/null（一个特殊的设备文件，会丢弃所有写入其中的数据）  \n",
    "        sys.stdout = open(os.devnull, 'w')  \n",
    "        sys.stderr = open(os.devnull, 'w')  \n",
    "  \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  \n",
    "        # 关闭重定向的文件对象  \n",
    "        sys.stdout.close()  \n",
    "        sys.stderr.close()  \n",
    "        # 恢复原始的标准输出和标准错误输出  \n",
    "        sys.stdout = self._original_stdout  \n",
    "        sys.stderr = self._original_stderr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pdf and split it into chunks\n",
    "loader = OnlinePDFLoader(\"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001813756/975b3e9b-268e-4798-a9e4-2a9a7c92dc10.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用递归字符文本拆分器将文档拆分成小块  \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)  \n",
    "all_splits = text_splitter.split_documents(data)  # 拆分文档  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用上下文管理器临时抑制输出，因为向量存储的创建可能会产生大量输出  \n",
    "with SuppressStdout():  \n",
    "    vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())  # 创建向量存储  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无限循环，等待用户输入查询  \n",
    "while True:  \n",
    "    query = input(\"\\nQuery: \")  # 获取用户输入的查询  \n",
    "    if query == \"exit\":  # 如果用户输入\"exit\"，则退出循环  \n",
    "        break  \n",
    "    if query.strip() == \"\":  # 如果用户输入为空，则继续循环  \n",
    "        continue  \n",
    "  \n",
    "    # 构建提示模板，用于指导LLM如何根据上下文和查询生成回答  \n",
    "    template = \"\"\"  \n",
    "    Use the following pieces of context to answer the question at the end.  \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.  \n",
    "    Use three sentences maximum and keep the answer as concise as possible.  \n",
    "    {context}  \n",
    "    Question: {question}  \n",
    "    Helpful Answer:  \n",
    "    \"\"\"  \n",
    "    QA_CHAIN_PROMPT = PromptTemplate(  \n",
    "        input_variables=[\"context\", \"question\"],  # 提示模板中的变量  \n",
    "        template=template,  # 模板字符串  \n",
    "    )  \n",
    "  \n",
    "    # 初始化Ollama模型，并设置回调管理器以将输出流式传输到标准输出  \n",
    "    llm = Ollama(model=\"llama3.1\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))  \n",
    "  \n",
    "    # 使用RetrievalQA链来实现检索式问答，其中向量存储作为检索器  \n",
    "    qa_chain = RetrievalQA.from_chain_type(  \n",
    "        llm=llm,  # LLM实例  \n",
    "        retriever=vectorstore.as_retriever(),  # 检索器  \n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},  # 链类型的关键字参数，包括提示模板  \n",
    "    )  \n",
    "  \n",
    "    # 执行检索式问答，并打印结果  \n",
    "    result = qa_chain({\"query\": query})  # 执行查询并获取"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
